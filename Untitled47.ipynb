{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPj31AGi9rAdQ2hItzZ2v2y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banti9021/-WhatsApp-LLM-Bot/blob/main/Untitled47.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CsEISyg-lkKx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9a4ace4b-cab6-45e2-b9cd-0ba91254238e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Screen Capture: Capture the game screen to get real-time input.\\n\\nTools: OpenCV for capturing and processing images.\\nGame  Use computer vision to interpret the game state.\\n\\nTools: OpenCV for image processing, possibly TensorFlow or PyTorch for object detection if needed.\\nDecision Making: Use AI algorithms to decide the next move based on the game state.\\n\\nTools: Reinforcement learning libraries like Stable Baselines or TensorFlow Agents.\\nAction Execution: Send commands to the game as a human player would.\\n\\nTools: PyAutoGUI or Selenium to automate keyboard and mouse input.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\"\"\"can artificial intelligence play games like html5 games similar to this- https://k4.games/\n",
        ".if how can you use concept of computer vision to prove this and tool you need to use\"\"\"\n",
        "\n",
        "'''Screen Capture: Capture the game screen to get real-time input.\n",
        "\n",
        "Tools: OpenCV for capturing and processing images.\n",
        "Game  Use computer vision to interpret the game state.\n",
        "\n",
        "Tools: OpenCV for image processing, possibly TensorFlow or PyTorch for object detection if needed.\n",
        "Decision Making: Use AI algorithms to decide the next move based on the game state.\n",
        "\n",
        "Tools: Reinforcement learning libraries like Stable Baselines or TensorFlow Agents.\n",
        "Action Execution: Send commands to the game as a human player would.\n",
        "\n",
        "Tools: PyAutoGUI or Selenium to automate keyboard and mouse input.'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python\n",
        "!pip install pyautogui\n",
        "!pip install mss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Rr5814gOzqAr",
        "outputId": "135b44b5-0f3b-441a-fe5f-8f866688345f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Requirement already satisfied: pyautogui in /usr/local/lib/python3.10/dist-packages (0.9.54)\n",
            "Requirement already satisfied: pymsgbox in /usr/local/lib/python3.10/dist-packages (from pyautogui) (1.0.9)\n",
            "Requirement already satisfied: pytweening>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from pyautogui) (1.2.0)\n",
            "Requirement already satisfied: pyscreeze>=0.1.21 in /usr/local/lib/python3.10/dist-packages (from pyautogui) (0.1.30)\n",
            "Requirement already satisfied: pygetwindow>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from pyautogui) (0.0.9)\n",
            "Requirement already satisfied: mouseinfo in /usr/local/lib/python3.10/dist-packages (from pyautogui) (0.1.3)\n",
            "Requirement already satisfied: python3-Xlib in /usr/local/lib/python3.10/dist-packages (from pyautogui) (0.15)\n",
            "Requirement already satisfied: pyrect in /usr/local/lib/python3.10/dist-packages (from pygetwindow>=0.0.5->pyautogui) (0.2.0)\n",
            "Requirement already satisfied: Pillow>=9.2.0 in /usr/local/lib/python3.10/dist-packages (from pyscreeze>=0.1.21->pyautogui) (9.4.0)\n",
            "Requirement already satisfied: pyperclip in /usr/local/lib/python3.10/dist-packages (from mouseinfo->pyautogui) (1.9.0)\n",
            "Requirement already satisfied: mss in /usr/local/lib/python3.10/dist-packages (9.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2  # OpenCV library for image processing\n",
        "import numpy as np\n",
        "import pyautogui  # For GUI automation\n",
        "import mss  # For screen capturing\n",
        "from time import sleep  # To add delays\n",
        "\n",
        "pyautogui.FAILSAFE = False  # Disable fail-safe\n",
        "\n",
        "sct = mss.mss()\n",
        "default_monitor = sct.monitors[1]\n",
        "\n",
        "def click_template_image(\n",
        "    template_image_path: str,\n",
        "    monitor=default_monitor,\n",
        "    threshold: float = 0.7,\n",
        "    number_of_clicks: int = 1,\n",
        "):\n",
        "    print(f\"{template_image_path} search\")\n",
        "    template_image = cv2.imread(template_image_path, 1)\n",
        "\n",
        "    # Capture the screen\n",
        "    game_screenshot = np.array(sct.grab((0, 0, monitor[\"width\"], monitor[\"height\"])))\n",
        "    game_screenshot = game_screenshot[:, :, :3]  # Remove alpha channel\n",
        "\n",
        "    # Perform template matching\n",
        "    search_result = cv2.matchTemplate(\n",
        "        game_screenshot, template_image, cv2.TM_CCOEFF_NORMED\n",
        "    )\n",
        "\n",
        "    # Extract matching results\n",
        "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(search_result)\n",
        "    print(max_val)\n",
        "\n",
        "    y_coords, x_coords = np.where(search_result >= threshold)\n",
        "    width_reset_multiplier = game_screenshot.shape[1] / monitor[\"width\"]\n",
        "    height_reset_multiplier = game_screenshot.shape[0] / monitor[\"height\"]\n",
        "    w = template_image.shape[1]\n",
        "    h = template_image.shape[0]\n",
        "\n",
        "    for idx in range(number_of_clicks):\n",
        "        if idx + 1 > len(x_coords):\n",
        "            continue\n",
        "\n",
        "        x, y = x_coords[idx], y_coords[idx]\n",
        "        x /= width_reset_multiplier\n",
        "        y /= height_reset_multiplier\n",
        "\n",
        "        x_c = int((x + x + w) // 2)\n",
        "        y_c = int((y + y + h) // 2)\n",
        "\n",
        "        pyautogui.click(x=x_c, y=y_c)\n",
        "        sleep(0.3)  # Delay for popups\n",
        "\n",
        "close_buttons = [\n",
        "    \"close.png\",\n",
        "    \"close_big.png\",\n",
        "    \"continue_level.png\",\n",
        "    \"yes_close_offer.png\",\n",
        "]\n",
        "\n",
        "valuables = [\n",
        "    \"cash.png\",\n",
        "    \"star.png\",\n",
        "    \"key.png\",\n",
        "]\n",
        "\n",
        "while True:\n",
        "    for valuable_image in valuables:\n",
        "        click_template_image(\"images/\" + valuable_image)\n",
        "\n",
        "        for close_button_image in close_buttons:\n",
        "            click_template_image(\"images/\" + close_button_image)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "xpRNI5lylr7I",
        "outputId": "b9efbe6e-8a84-4f8e-d9f1-d3d50a5239d0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'DISPLAY'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d7bc6ed5d0e6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m  \u001b[0;31m# OpenCV library for image processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyautogui\u001b[0m  \u001b[0;31m# For GUI automation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmss\u001b[0m  \u001b[0;31m# For screen capturing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msleep\u001b[0m  \u001b[0;31m# To add delays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyautogui/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmouseinfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmouseInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mouseinfo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DISPLAY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_linuxPosition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'DISPLAY'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"is ai animation is possible .if yes what kind of ai/ml tools can be used for making videos like .\n",
        "also ,let us know how can we develop some basic tools for same\"\"\"\n",
        "\n",
        "\n",
        "Absolutely, AI animation is real and growing rapidly.\n",
        "\n",
        "Tools and Techniques\n",
        "Generative AI:\n",
        "\n",
        "RunwayML: User-friendly for creatives to make AI-generated videos.\n",
        "DALL-E and Stable Diffusion: Generate images that can be sequenced into animations.\n",
        "Motion Capture:\n",
        "\n",
        "DeepMotion: Converts video into 3D animations.\n",
        "Rokoko: Offers tools for capturing and retargeting motion data.\n",
        "Video Editing Software:\n",
        "\n",
        "Blender: Open-source 3D creation with Python scripting for AI integration.\n",
        "Adobe After Effects: Integrate AI plugins for special effects.\n",
        "Style Transfer:\n",
        "\n",
        "Tools like Artbreeder allow style modifications on images for animation.\n",
        "How to Develop Basic AI Animation Tools\n",
        "Setup:\n",
        "\n",
        "Install Python and libraries: OpenCV, TensorFlow, or PyTorch.\n",
        "Get familiar with video editing software like Blender or Adobe After Effects.\n",
        "Basic Animation Pipeline:\n",
        "\n",
        "Data Collection: Gather image datasets relevant to your project.\n",
        "Training: Use GANs to learn patterns and generate new frames.\n",
        "Integration: Use OpenPose for tracking motion and apply it to 3D models.\n",
        "Video Creation: Compile generated frames into a video with OpenCV or Blender.\n",
        "Practical Steps:\n",
        "\n",
        "Start with a small project like a stylized video clip using neural style transfer.\n",
        "Experiment with motion capture data to animate a simple character.\n",
        "Use AI tools to automate repetitive animation tasks, like background generation.\n",
        "Learning and Community\n",
        "Online Tutorials: Platforms like YouTube have practical guides for AI in animation.\n",
        "Forums and Groups: Engage with communities on Discord or Reddit for support and feedback.\n",
        "Workshops and Courses: Attend workshops to learn from experts in AI animation.\n",
        "Real-Life Applications\n",
        "Create short animated films or music videos with AI-generated effects.\n",
        "Develop interactive media or games with procedurally generated content.\n",
        "Automate parts of the animation workflow to speed up production.\n",
        "AI in animation offers creative possibilities with tools that are increasingly accessible to individuals and small teams. Start small, experiment, and gradually build more complex projects.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vy1UHJjGnWkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bjOF_NjUx3fN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}